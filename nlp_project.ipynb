{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis for Financial News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Camille\n",
      "[nltk_data]     Leempoels\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# cleaning\n",
    "import unidecode\n",
    "\n",
    "# stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# vocab\n",
    "from collections import Counter\n",
    "\n",
    "# models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', encoding='latin-1', names=['sentiment','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text\n",
       "0   neutral  According to Gran , the company has no plans t...\n",
       "1   neutral  Technopolis plans to develop in stages an area...\n",
       "2  negative  The international electronic industry company ...\n",
       "3  positive  With the new production plant the company woul...\n",
       "4  positive  According to the company 's updated strategy f..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation and special caracters\n",
    "data['text'] = data['text'].str.replace('[^\\w\\s]', '', regex=True)\n",
    "data['text'] = data['text'].str.replace('_', ' ')\n",
    "data['text'] = data['text'].astype('unicode')\n",
    "data['text'] = data['text'].transform(lambda x: unidecode.unidecode(x))\n",
    "\n",
    "# Lowercase the text\n",
    "data['text'] = data['text'].str.lower()\n",
    "\n",
    "# Removing numbers\n",
    "data['text'] = data['text'].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "# Removing stop words\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "data['text'] = data['text'].apply(lambda x: [item for item in x.split() if item not in nltk_stopwords])\n",
    "\n",
    "# Lemmatization and/or Expand Contractions ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_set_, _test_set_ = train_test_split(data, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the vocabulary from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the training set : 46400\n",
      "Number of unique words in the training set : 8337\n",
      "Most frequent words : [('eur', 1057), ('company', 680), ('mn', 475), ('said', 437), ('finnish', 405), ('sales', 354), ('million', 335), ('profit', 325), ('net', 323), ('finland', 278)]\n"
     ]
    }
   ],
   "source": [
    "# Extract all the words and count them\n",
    "vocab = Counter([word for sentence in _train_set_['text'].values.tolist() for word in sentence])\n",
    "\n",
    "print('Number of words in the training set : ' + str(sum(vocab.values())))\n",
    "print('Number of unique words in the training set : ' + str(len(list(vocab))))\n",
    "\n",
    "# Sort the words\n",
    "vocab = vocab.most_common()\n",
    "\n",
    "print('Most frequent words : ' + str(vocab[0:10]))\n",
    "\n",
    "# Convert the vocabulary to a Python Dict\n",
    "vocab = dict(vocab)\n",
    "\n",
    "# encode words as integers\n",
    "sparse_vocab = {word:i for i, word in enumerate(vocab, 1)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(data):\n",
    "    # encode sentences \n",
    "    # set -1 if the word is not in the vocabulary\n",
    "    data['text_encoded'] = data['text'].apply(\n",
    "        lambda sentence : [sparse_vocab.get(word, -1) for word in sentence]\n",
    "    )\n",
    "\n",
    "    # if sentences are extremely short or long, drop them\n",
    "    data['length'] = data['text_encoded'].apply(lambda sentence : len(sentence))\n",
    "    limit_low, limit_high  = data['length'].quantile(q=[0.01, 0.99]).values\n",
    "    data = data[(data['length'] <= limit_high) & (data['length'] >= limit_low)].copy()\n",
    "\n",
    "    # pad sentences so that each vector has the same length\n",
    "    max_len = int(limit_high)\n",
    "    data['text_encoded'] = data.apply(lambda row: row['text_encoded'] + [0] * (max_len - row['length']), axis=1)\n",
    "    data['length'] = data['text_encoded'].apply(lambda sentence : len(sentence))\n",
    "\n",
    "    if sum(data['length'] != max_len):\n",
    "        print('WARNING')\n",
    "\n",
    "    return data\n",
    "\n",
    "train_set_txt = encode_text(_train_set_.copy())\n",
    "test_set_txt = encode_text(_test_set_.copy())\n",
    "\n",
    "train_set_labels = OneHotEncoder(handle_unknown='ignore', sparse_output=False).fit_transform(train_set_txt['sentiment'].values.reshape(-1, 1))\n",
    "test_set_labels = OneHotEncoder(handle_unknown='ignore', sparse_output=False).fit_transform(test_set_txt['sentiment'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloaders and Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "\n",
    "train_set = TensorDataset(\n",
    "    torch.tensor(np.vstack(train_set_txt['text_encoded'])), \n",
    "    torch.tensor(train_set_labels)\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_set, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reccurent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_model(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_dim, embedding_dim, hidden_dim):\n",
    "        super(RNN_model, self).__init__()\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_dim, embedding_dim)\n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        # fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, 3)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, txt):\n",
    "        emb_txt = self.embedding(txt.T)\n",
    "        rnn_out, rnn_hidden = self.rnn(emb_txt)\n",
    "        fc_out = self.fc(rnn_hidden.squeeze(0))\n",
    "        output = self.softmax(fc_out)\n",
    "\n",
    "        return fc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_DIM = len(vocab) + 1\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 32\n",
    "LEARNING_RATE = 1e-2\n",
    "\n",
    "model = RNN_model(VOCAB_DIM, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN_model(\n",
      "  (embedding): Embedding(8338, 100)\n",
      "  (rnn): RNN(100, 32)\n",
      "  (fc): Linear(in_features=32, out_features=3, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([50, 3])\n",
      "torch.Size([50, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[232], line 30\u001b[0m\n\u001b[0;32m     17\u001b[0m         loss \u001b[39m=\u001b[39m criterion(pred, label)\n\u001b[0;32m     19\u001b[0m     \u001b[39m#     acc = binary_accuracy(predictions, batch.label)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m         \n\u001b[0;32m     21\u001b[0m     \u001b[39m#     loss.backward()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \n\u001b[0;32m     28\u001b[0m     \u001b[39m# return epoch_loss / len(iterator), epoch_acc / len(iterator)\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m train_loop(train_dataloader, model, optimizer, criterion)\n",
      "Cell \u001b[1;32mIn[232], line 17\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, optimizer, criterion)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mprint\u001b[39m(pred\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     16\u001b[0m \u001b[39mprint\u001b[39m(label\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 17\u001b[0m loss \u001b[39m=\u001b[39m criterion(pred, label)\n",
      "File \u001b[1;32mc:\\Users\\Camille Leempoels\\.conda\\envs\\finance\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Camille Leempoels\\.conda\\envs\\finance\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1120\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1120\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1121\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\Camille Leempoels\\.conda\\envs\\finance\\lib\\site-packages\\torch\\nn\\functional.py:2824\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2822\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2823\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 2824\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "def train_loop(dataloader, model, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    count = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for txt, label in dataloader:\n",
    "        count = count + 1\n",
    "        print(count)\n",
    "\n",
    "        optimizer.zero_grad()  \n",
    "        pred = model(txt)\n",
    "        print(pred.shape)\n",
    "        print(label.shape)\n",
    "        loss = criterion(pred, label)\n",
    "        \n",
    "    #     acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "    #     loss.backward()\n",
    "        \n",
    "    #     optimizer.step()\n",
    "        \n",
    "    #     epoch_loss += loss.item()\n",
    "    #     epoch_acc += acc.item()\n",
    "        \n",
    "    # return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "train_loop(train_dataloader, model, optimizer, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('finance')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87b7d1b632ea9fc659b38cb235882dc6d49eb3c3a9bfcc80fa5458e44c02fe83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
