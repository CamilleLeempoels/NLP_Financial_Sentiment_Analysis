{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis for Financial News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Camille\n",
      "[nltk_data]     Leempoels\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unidecode\n",
    "import torch\n",
    "import nltk\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data_ = pd.read_csv('data.csv', encoding='latin-1', names=['sentiment','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text\n",
       "0   neutral  According to Gran , the company has no plans t...\n",
       "1   neutral  Technopolis plans to develop in stages an area...\n",
       "2  negative  The international electronic industry company ...\n",
       "3  positive  With the new production plant the company woul...\n",
       "4  positive  According to the company 's updated strategy f..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = _data_.copy()\n",
    "\n",
    "# Removing punctuation and special caracters\n",
    "data['text'] = data['text'].str.replace('[^\\w\\s]', '', regex=True)\n",
    "data['text'] = data['text'].str.replace('_', ' ')\n",
    "data['text'] = data['text'].astype('unicode')\n",
    "data['text'] = data['text'].transform(lambda x: unidecode.unidecode(x))\n",
    "\n",
    "# Lowercase the text\n",
    "data['text'] = data['text'].str.lower()\n",
    "\n",
    "# Removing numbers\n",
    "data['text'] = data['text'].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "# Removing stop words\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "data['text'] = data['text'].apply(lambda x: [item for item in x.split() if item not in nltk_stopwords])\n",
    "\n",
    "# Lemmatization and/or Expand Contractions ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the training set : 46400\n",
      "Number of unique words in the training set : 8337\n",
      "Most frequent words : [('eur', 1057), ('company', 680), ('mn', 475), ('said', 437), ('finnish', 405), ('sales', 354), ('million', 335), ('profit', 325), ('net', 323), ('finland', 278)]\n"
     ]
    }
   ],
   "source": [
    "# Extract all the words and count them\n",
    "vocab = Counter([word for sentence in data_train['text'].values.tolist() for word in sentence])\n",
    "\n",
    "print('Number of words in the training set : ' + str(sum(vocab.values())))\n",
    "print('Number of unique words in the training set : ' + str(len(list(vocab))))\n",
    "\n",
    "# Sort the words\n",
    "vocab = vocab.most_common()\n",
    "\n",
    "print('Most frequent words : ' + str(vocab[0:10]))\n",
    "\n",
    "# Convert the vocabulary to a Python Dict\n",
    "vocab = dict(vocab)\n",
    "\n",
    "# encode words as integers\n",
    "sparse_vocab = {word:i for i, word in enumerate(vocab, 1)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode sentences\n",
    "text_train = []\n",
    "data_train['text_encoded'] = data_train['text'].apply(\n",
    "    lambda sentence : [sparse_vocab[word] for word in sentence]\n",
    ")\n",
    "\n",
    "# encode labels\n",
    "data_train['label'] = data_train['sentiment'].replace(['positive', 'neutral', 'negative'], [1, 0, -1])\n",
    "\n",
    "# check for outliers\n",
    "data_train['length'] = data_train['text_encoded'].apply(lambda sentence : len(sentence))\n",
    "limit_low, limit_high  = data_train['length'].quantile(q=[0.01, 0.99]).values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train.iloc[data_train['length'].clip(limit_low, limit_high).index.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encoding (i'm not using it, just a try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding (sklearn preprocessing)\n",
    "one_hot_enc = MultiLabelBinarizer().fit(data_train.text)\n",
    "one_hot_txt = one_hot_enc.transform(data_train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words : 8337\n",
      "Position of \"company\" : 1342\n",
      "One Hot representation of \"company\" : [[0 0 0 ... 0 0 0]]\n",
      "1342-th element of the One Hot encoding of \"company\" : 1\n"
     ]
    }
   ],
   "source": [
    "# Toy example\n",
    "print('Number of unique words : ' + str(len(one_hot_enc.classes_)))\n",
    "idx = np.where(one_hot_enc.classes_ == 'company')[0][0]\n",
    "print('Position of \"company\" : ' + str(idx))\n",
    "print('One Hot representation of \"company\" : ' + str(one_hot_enc.transform([['company']])))\n",
    "print(str(idx) + '-th element of the One Hot encoding of \"company\" : ' + str(one_hot_enc.transform([['company']])[0,idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('finance')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87b7d1b632ea9fc659b38cb235882dc6d49eb3c3a9bfcc80fa5458e44c02fe83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
