{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis for Financial News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Camille\n",
      "[nltk_data]     Leempoels\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# cleaning\n",
    "import unidecode\n",
    "\n",
    "# stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# vocab\n",
    "from collections import Counter\n",
    "\n",
    "# models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data_ = pd.read_csv('data.csv', encoding='latin-1', names=['sentiment','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text\n",
       "0   neutral  According to Gran , the company has no plans t...\n",
       "1   neutral  Technopolis plans to develop in stages an area...\n",
       "2  negative  The international electronic industry company ...\n",
       "3  positive  With the new production plant the company woul...\n",
       "4  positive  According to the company 's updated strategy f..."
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = _data_.copy()\n",
    "\n",
    "# Removing punctuation and special caracters\n",
    "data['text'] = data['text'].str.replace('[^\\w\\s]', '', regex=True)\n",
    "data['text'] = data['text'].str.replace('_', ' ')\n",
    "data['text'] = data['text'].astype('unicode')\n",
    "data['text'] = data['text'].transform(lambda x: unidecode.unidecode(x))\n",
    "\n",
    "# Lowercase the text\n",
    "data['text'] = data['text'].str.lower()\n",
    "\n",
    "# Removing numbers\n",
    "data['text'] = data['text'].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "# Removing stop words\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "data['text'] = data['text'].apply(lambda x: [item for item in x.split() if item not in nltk_stopwords])\n",
    "\n",
    "# Lemmatization and/or Expand Contractions ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the vocabulary from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the training set : 46400\n",
      "Number of unique words in the training set : 8337\n",
      "Most frequent words : [('eur', 1057), ('company', 680), ('mn', 475), ('said', 437), ('finnish', 405), ('sales', 354), ('million', 335), ('profit', 325), ('net', 323), ('finland', 278)]\n"
     ]
    }
   ],
   "source": [
    "# Extract all the words and count them\n",
    "vocab = Counter([word for sentence in data_train['text'].values.tolist() for word in sentence])\n",
    "\n",
    "print('Number of words in the training set : ' + str(sum(vocab.values())))\n",
    "print('Number of unique words in the training set : ' + str(len(list(vocab))))\n",
    "\n",
    "# Sort the words\n",
    "vocab = vocab.most_common()\n",
    "\n",
    "print('Most frequent words : ' + str(vocab[0:10]))\n",
    "\n",
    "# Convert the vocabulary to a Python Dict\n",
    "vocab = dict(vocab)\n",
    "\n",
    "# encode words as integers\n",
    "sparse_vocab = {word:i for i, word in enumerate(vocab, 1)} \n",
    "\n",
    "# encode sentences\n",
    "text_train = []\n",
    "data_train['text_encoded'] = data_train['text'].apply(\n",
    "    lambda sentence : [sparse_vocab[word] for word in sentence]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if sentences are extremely short or long, drop them\n",
    "data_train['length'] = data_train['text_encoded'].apply(lambda sentence : len(sentence))\n",
    "limit_low, limit_high  = data_train['length'].quantile(q=[0.01, 0.99]).values\n",
    "data_train = data_train[(data_train['length'] <= limit_high) & (data_train['length'] >= limit_low)].copy()\n",
    "\n",
    "# pad sentences so that each vector has the same length\n",
    "max_len = int(limit_high)\n",
    "data_train['text_encoded'] = data_train.apply(lambda row: row['text_encoded'] + [0] * (max_len - row['length']), axis=1)\n",
    "data_train['length'] = data_train['text_encoded'].apply(lambda sentence : len(sentence))\n",
    "\n",
    "if sum(data_train['length'] != max_len):\n",
    "    print('WARNING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode labels\n",
    "data_train['label'] = data_train['sentiment'].replace(['positive', 'neutral', 'negative'], [1, 0, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders and Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "\n",
    "train_set = TensorDataset(\n",
    "    torch.tensor(np.vstack(data_train['text_encoded'])), \n",
    "    torch.tensor(np.vstack(data_train['label']))\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_set, \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('finance')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87b7d1b632ea9fc659b38cb235882dc6d49eb3c3a9bfcc80fa5458e44c02fe83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
