{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis for Financial News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Camille\n",
      "[nltk_data]     Leempoels\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unidecode\n",
    "import torch\n",
    "import nltk\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data_ = pd.read_csv('data.csv', encoding='latin-1', names=['sentiment','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text\n",
       "0   neutral  According to Gran , the company has no plans t...\n",
       "1   neutral  Technopolis plans to develop in stages an area...\n",
       "2  negative  The international electronic industry company ...\n",
       "3  positive  With the new production plant the company woul...\n",
       "4  positive  According to the company 's updated strategy f..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = _data_.copy()\n",
    "\n",
    "# Removing punctuation and special caracters\n",
    "data['text'] = data['text'].str.replace('[^\\w\\s]', '', regex=True)\n",
    "data['text'] = data['text'].str.replace('_', ' ')\n",
    "data['text'] = data['text'].astype('unicode')\n",
    "data['text'] = data['text'].transform(lambda x: unidecode.unidecode(x))\n",
    "\n",
    "# Lowercase the text\n",
    "data['text'] = data['text'].str.lower()\n",
    "\n",
    "# Removing numbers\n",
    "data['text'] = data['text'].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "# Removing stop words\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "data['text'] = data['text'].apply(lambda x: [item for item in x.split() if item not in nltk_stopwords])\n",
    "\n",
    "# Lemmatization and/or Expand Contractions ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words : 58223\n",
      "Number of unique words : 9361\n",
      "Most frequent words : [('eur', 1310), ('company', 848), ('mn', 593), ('said', 544), ('finnish', 512), ('sales', 453), ('million', 441), ('net', 412), ('profit', 409), ('finland', 337)]\n"
     ]
    }
   ],
   "source": [
    "# Extract all the words and count them\n",
    "vocab = Counter([word for sentence in data['text'].values.tolist() for word in sentence])\n",
    "\n",
    "print('Number of words : ' + str(sum(vocab.values())))\n",
    "print('Number of unique words : ' + str(len(list(vocab))))\n",
    "\n",
    "# Sort the words\n",
    "vocab = vocab.most_common()\n",
    "\n",
    "print('Most frequent words : ' + str(vocab[0:10]))\n",
    "\n",
    "# Convert the vocabulary to a Python Dict\n",
    "# vocab = dict(vocab)\n",
    "\n",
    "# encode words as integers\n",
    "# sparse_vocab = {word:i for i, word in enumerate(vocab, 1)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding (sklearn preprocessing)\n",
    "one_hot_enc = MultiLabelBinarizer().fit(data.text)\n",
    "one_hot_txt = one_hot_enc.transform(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words : 9361\n",
      "Position of \"company\" : 1483\n",
      "One Hot representation of \"company\" : [[0 0 0 ... 0 0 0]]\n",
      "1483-th element of the One Hot encoding of \"company\" : 1\n"
     ]
    }
   ],
   "source": [
    "# Toy example\n",
    "print('Number of unique words : ' + str(len(one_hot_enc.classes_)))\n",
    "idx = np.where(one_hot_enc.classes_ == 'company')[0][0]\n",
    "print('Position of \"company\" : ' + str(idx))\n",
    "print('One Hot representation of \"company\" : ' + str(one_hot_enc.transform([['company']])))\n",
    "print(str(idx) + '-th element of the One Hot encoding of \"company\" : ' + str(one_hot_enc.transform([['company']])[0,idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('finance')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87b7d1b632ea9fc659b38cb235882dc6d49eb3c3a9bfcc80fa5458e44c02fe83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
